{
  "": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 6
  },
  "faq/ask_veterinarian": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "ask_female_castration": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 1
  },
  "[sintomas](question)": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 2
  },
  "utter_goodbye": {
    "precision": 1.0,
    "recall": 0.875,
    "f1-score": 0.9333333333333333,
    "support": 8
  },
  "thank_you_intent": {
    "precision": 0.8571428571428571,
    "recall": 1.0,
    "f1-score": 0.923076923076923,
    "support": 6
  },
  "utter_ask_castration": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "adopt_intent": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 2
  },
  "[tratamento](question)": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "utter_faq": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 4
  },
  "action_reset_pet_slots": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 1
  },
  "[sarna](disease)": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 2
  },
  "utter_anything_else": {
    "precision": 1.0,
    "recall": 0.9,
    "f1-score": 0.9473684210526316,
    "support": 10
  },
  "askaction": {
    "precision": 0.75,
    "recall": 1.0,
    "f1-score": 0.8571428571428571,
    "support": 3
  },
  "action_answer_disease": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 3
  },
  "action_listen": {
    "precision": 1.0,
    "recall": 0.8536585365853658,
    "f1-score": 0.9210526315789475,
    "support": 41
  },
  "utter_about_adopt": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 2
  },
  "utter_welcome": {
    "precision": 1.0,
    "recall": 0.8571428571428571,
    "f1-score": 0.923076923076923,
    "support": 7
  },
  "deny_intent": {
    "precision": 0.8888888888888888,
    "recall": 1.0,
    "f1-score": 0.9411764705882353,
    "support": 8
  },
  "action_utter_greet": {
    "precision": 0.8571428571428571,
    "recall": 1.0,
    "f1-score": 0.923076923076923,
    "support": 6
  },
  "faq": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 3
  },
  "utter_searching": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 1
  },
  "utter_ask_female_castration": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "utter_greet": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 2
  },
  "greet_intent": {
    "precision": 0.875,
    "recall": 1.0,
    "f1-score": 0.9333333333333333,
    "support": 7
  },
  "affirm_intent": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 2
  },
  "ask_castration_intent": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 1
  },
  "utter_how_can_i_help": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "greet": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 1
  },
  "utter_need_information": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "action_scrapping": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 1
  },
  "pet_form": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "[cinomose](disease)": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "micro avg": {
    "precision": 0.9583333333333334,
    "recall": 0.8333333333333334,
    "f1-score": 0.8914728682170543,
    "support": 138
  },
  "macro avg": {
    "precision": 0.7341871091871093,
    "recall": 0.741993981628128,
    "f1-score": 0.7364435701897003,
    "support": 138
  },
  "weighted avg": {
    "precision": 0.8679117782378651,
    "recall": 0.8333333333333334,
    "f1-score": 0.8469285795641968,
    "support": 138
  },
  "conversation_accuracy": {
    "accuracy": 0.5555555555555556,
    "correct": 5,
    "with_warnings": 0,
    "total": 9
  }
}